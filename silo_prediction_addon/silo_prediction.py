#!/usr/bin/env python3
"""
Silo Prediction Home Assistant Add-on
Intelligens sil√≥ ki√ºr√ºl√©si el≈ërejelz√©s line√°ris regresszi√≥val
"""

import os
import time
import logging
import requests
import numpy as np
from datetime import datetime, timedelta
from typing import List, Tuple, Dict, Optional
from scipy import stats

# Logging be√°ll√≠t√°sa
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/silo_prediction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SiloPredictionAddon:
    def __init__(self):
        self.ha_url = os.getenv('HA_URL', 'http://supervisor/core')
        self.ha_token = os.getenv('HA_TOKEN', os.getenv('SUPERVISOR_TOKEN'))
        self.entity_id = os.getenv('ENTITY_ID', 'sensor.cfm_3_hall_modbus_1_lp7516_merleg_suly')
        self.sensor_name = os.getenv('SENSOR_NAME', 'Silo Prediction')
        self.refill_threshold = int(os.getenv('REFILL_THRESHOLD', '1000'))
        self.max_capacity = int(os.getenv('MAX_CAPACITY', '20000'))
        self.prediction_days = int(os.getenv('PREDICTION_DAYS', '10'))
        self.update_interval = int(os.getenv('UPDATE_INTERVAL', '3600'))

        self.headers = {
            'Authorization': f'Bearer {self.ha_token}',
            'Content-Type': 'application/json'
        }

        logger.info("üöÄ Silo Prediction Add-on ind√≠tva")
        logger.info(f"Home Assistant URL: {self.ha_url}")
        logger.info(f"Entity ID: {self.entity_id}")

        # Debug: Check if token is available
        if not self.ha_token:
            logger.error("‚ùå SUPERVISOR_TOKEN vagy HA_TOKEN nincs be√°ll√≠tva!")
        else:
            logger.info(f"‚úÖ Token hossza: {len(self.ha_token)} karakter")

    def get_historical_data(self, days: int = 10) -> List[Tuple[datetime, float]]:
        """T√∂rt√©neti adatok lek√©r√©se a Home Assistant API-b√≥l"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)

        logger.info(f"üìä Adatok lek√©r√©se: {start_time} - {end_time}")

        url = f"{self.ha_url}/api/history/period/{start_time.isoformat()}"
        params = {
            'filter_entity_id': self.entity_id,
            'end_time': end_time.isoformat()
        }

        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()

            if not data or not data[0]:
                logger.warning("‚ùå Nincs adat a v√°laszban")
                return []

            # Adatok feldolgoz√°sa
            processed_data = []
            for entry in data[0]:
                try:
                    timestamp = datetime.fromisoformat(entry['last_changed'].replace('Z', '+00:00'))
                    timestamp = timestamp.replace(tzinfo=None)  # Elt√°vol√≠tjuk a timezone info-t

                    state = entry.get('state', '0')
                    if state in ['unknown', 'unavailable', 'null', None]:
                        continue

                    weight = float(state)
                    if 0 <= weight <= 50000:  # √ârv√©nyes tartom√°ny
                        processed_data.append((timestamp, weight))

                except (ValueError, KeyError, TypeError) as e:
                    continue

            # Id≈ërend szerint rendez√©s
            processed_data.sort(key=lambda x: x[0])

            logger.info(f"‚úÖ {len(processed_data)} adatpont bet√∂ltve")
            return processed_data

        except requests.RequestException as e:
            logger.error(f"‚ùå API hiba: {e}")
            return []

    def sample_hourly_data(self, data: List[Tuple[datetime, float]]) -> List[Tuple[datetime, float]]:
        """√ìr√°nk√©nti mintav√©telez√©s az adatokb√≥l"""
        if not data:
            return []

        hourly_data = []
        current_hour = None
        hour_values = []

        for timestamp, weight in data:
            hour = timestamp.replace(minute=0, second=0, microsecond=0)

            if current_hour is None:
                current_hour = hour

            if hour == current_hour:
                hour_values.append(weight)
            else:
                if hour_values:
                    avg_weight = np.mean(hour_values)
                    hourly_data.append((current_hour, avg_weight))
                current_hour = hour
                hour_values = [weight]

        # Utols√≥ √≥ra
        if hour_values and current_hour:
            avg_weight = np.mean(hour_values)
            hourly_data.append((current_hour, avg_weight))

        logger.info(f"üìà {len(hourly_data)} √≥r√°nk√©nti adatpont mintav√©telezve")
        return hourly_data

    def detect_refills(self, data: List[Tuple[datetime, float]]) -> List[Tuple[datetime, float]]:
        """
        Felt√∂lt√©sek detekt√°l√°sa √©s csak az utols√≥ felt√∂lt√©s UT√ÅNI adatok megtart√°sa.
        Ez az√©rt fontos, mert csak az utols√≥ felt√∂lt√©s ut√°n tudjuk pontosan el≈ërejelezni az √ºr√ºl√©st.
        """
        if len(data) < 2:
            return data

        last_refill_index = -1

        # Keress√ºk meg az utols√≥ felt√∂lt√©s index√©t
        for i in range(1, len(data)):
            prev_weight = data[i-1][1]
            curr_weight = data[i][1]
            weight_change = curr_weight - prev_weight

            # Ha a s√∫ly 3000kg-n√°l t√∂bbel n≈ëtt, az felt√∂lt√©s
            # (√ìr√°nk√©nti √°tlagol√°s ut√°n is detekt√°lhat√≥ legyen)
            if weight_change > 3000:
                logger.info(f"üîÑ Felt√∂lt√©s detekt√°lva: {data[i-1][0]} -> {data[i][0]}, "
                           f"S√∫lyv√°ltoz√°s: +{weight_change:.0f}kg")
                last_refill_index = i

        # Ha volt felt√∂lt√©s, csak az utols√≥ felt√∂lt√©s ut√°ni adatokat tartjuk meg
        if last_refill_index >= 0:
            cleaned_data = data[last_refill_index:]
            logger.info(f"‚úÖ Utols√≥ felt√∂lt√©s ut√°n: {len(cleaned_data)} adatpont ({data[last_refill_index][0]})")
        else:
            cleaned_data = data
            logger.info(f"‚úÖ Nem volt felt√∂lt√©s, {len(cleaned_data)} adatpont haszn√°lva")

        return cleaned_data

    def calculate_prediction(self, data: List[Tuple[datetime, float]]) -> Optional[Dict]:
        """El≈ërejelz√©s k√©sz√≠t√©se line√°ris regresszi√≥val"""
        if len(data) < 24:
            logger.warning(f"‚ùå Nincs el√©g adat az el≈ërejelz√©shez (minimum 24 √≥ra kell, {len(data)} van)")
            return None

        # Id≈ëpontok √©s s√∫lyok sz√©tv√°laszt√°sa
        timestamps = [t for t, w in data]
        weights = [w for t, w in data]

        # Unix timestamp-ekk√© konvert√°l√°s (√≥r√°k)
        start_time = timestamps[0]
        hours = [(t - start_time).total_seconds() / 3600 for t in timestamps]

        # Line√°ris regresszi√≥
        slope, intercept, r_value, p_value, std_err = stats.linregress(hours, weights)

        r_squared = r_value ** 2

        logger.info(f"üìâ Regresszi√≥: meredeks√©g={slope:.2f} kg/√≥ra, R¬≤={r_squared:.4f}")

        current_hours = hours[-1]
        current_weight = weights[-1]

        # Sz√°m√≠tsuk ki, mikor lesz 0 kg (mindig, f√ºggetlen√ºl a trendd≈ël)
        # y = slope * x + intercept
        # 0 = slope * x + intercept
        # x = -intercept / slope

        if abs(slope) < 0.01:
            # Ha a meredeks√©g k√∂zel nulla, nincs √©rtelmes el≈ërejelz√©s
            logger.warning("‚ö†Ô∏è K√∂zel nulla meredeks√©g, nincs trend")
            return {
                'prediction_date': None,
                'days_until_empty': None,
                'slope': slope,
                'r_squared': r_squared,
                'current_weight': current_weight,
                'status': 'no_trend'
            }

        # Ellen≈ërizz√ºk a trend ir√°ny√°t
        if slope >= -0.1:
            # A sil√≥ nem √ºr√ºl (t√∂lt≈ëdik vagy stabil)
            logger.info("‚ö†Ô∏è A sil√≥ nem √ºr√ºl (pozit√≠v vagy nulla trend)")
            return {
                'prediction_date': None,
                'days_until_empty': None,
                'slope': slope,
                'r_squared': r_squared,
                'current_weight': current_weight,
                'threshold': 0,
                'status': 'filling' if slope > 0 else 'stable'
            }

        # Negat√≠v slope - a sil√≥ √ºr√ºl
        # H√°ny √≥ra m√∫lva lesz 0 kg?
        hours_to_zero = -intercept / slope
        hours_from_now = hours_to_zero - current_hours

        # Ellen≈ërizz√ºk, hogy √©rtelmes-e az el≈ërejelz√©s
        if hours_from_now < 0:
            logger.warning("‚ö†Ô∏è A sz√°m√≠t√°s szerint m√°r ki√ºr√ºlt volna (hib√°s adat)")
            return {
                'prediction_date': None,
                'days_until_empty': None,
                'slope': slope,
                'r_squared': r_squared,
                'current_weight': current_weight,
                'threshold': 0,
                'status': 'error'
            }

        days_until = hours_from_now / 24

        if days_until > 365:
            logger.info(f"‚ö†Ô∏è T√∫l t√°voli el≈ërejelz√©s: {days_until:.0f} nap")
            return {
                'prediction_date': None,
                'days_until_empty': None,
                'slope': slope,
                'r_squared': r_squared,
                'current_weight': current_weight,
                'threshold': 0,
                'status': 'too_far'
            }

        # √ârv√©nyes √ºr√ºl√©si el≈ërejelz√©s
        prediction_datetime = datetime.now() + timedelta(hours=hours_from_now)

        # Form√°zott d√°tum: YYYY-MM-DD HH:MM (m√°sodperc n√©lk√ºl)
        formatted_date = prediction_datetime.strftime('%Y-%m-%d %H:%M')

        logger.info(f"üìÖ 0 kg el≈ërejelz√©s: {formatted_date}")
        logger.info(f"‚è±Ô∏è H√°tral√©v≈ë id≈ë: {days_until:.1f} nap")

        return {
            'prediction_date': formatted_date,  # Form√°zott string, nem ISO
            'days_until_empty': round(days_until, 2),
            'slope': round(slope, 2),
            'r_squared': round(r_squared, 4),
            'current_weight': round(current_weight, 0),
            'threshold': 0,
            'status': 'emptying'
        }

    def update_sensor(self, prediction_data: Dict):
        """Home Assistant szenzor friss√≠t√©se"""
        if not prediction_data:
            logger.warning("‚ùå Nincs el≈ërejelz√©si adat a szenzor friss√≠t√©shez")
            return

        sensor_entity_id = f"sensor.{self.sensor_name.lower().replace(' ', '_')}"

        # A state √©rt√©ke: ha van prediction_date, akkor azt haszn√°ljuk (timestamp form√°tumban)
        # K√ºl√∂nben a status √©rt√©k√©t
        prediction_date = prediction_data.get('prediction_date')
        status = prediction_data.get('status', 'unknown')

        if prediction_date:
            # ISO form√°tum√∫ d√°tumot haszn√°lunk state-k√©nt
            state = prediction_date
        else:
            # Ha nincs d√°tum, a status-t haszn√°ljuk
            state = status

        attributes = {
            'prediction_date': prediction_date,
            'days_until_empty': prediction_data.get('days_until_empty'),
            'slope_kg_per_hour': prediction_data.get('slope'),
            'r_squared': prediction_data.get('r_squared'),
            'current_weight_kg': prediction_data.get('current_weight'),
            'threshold_kg': prediction_data.get('threshold'),
            'status': status,
            'friendly_name': self.sensor_name,
            'icon': 'mdi:silo'
        }

        url = f"{self.ha_url}/api/states/{sensor_entity_id}"
        payload = {
            'state': state,
            'attributes': attributes
        }

        try:
            logger.debug(f"Szenzor friss√≠t√©s URL: {url}")
            logger.debug(f"Payload: {payload}")
            response = requests.post(url, headers=self.headers, json=payload, timeout=10)
            response.raise_for_status()
            logger.info(f"‚úÖ Szenzor friss√≠tve: {sensor_entity_id} = {state} nap")
        except requests.RequestException as e:
            logger.error(f"‚ùå Szenzor friss√≠t√©si hiba: {e}")
            if hasattr(e.response, 'text'):
                logger.error(f"V√°lasz: {e.response.text}")

    def run(self):
        """F≈ë fut√°si ciklus"""
        logger.info("üîÑ Prediction szolg√°ltat√°s ind√≠tva")

        while True:
            try:
                # 1. Adatok lek√©r√©se
                raw_data = self.get_historical_data(days=self.prediction_days)

                if not raw_data:
                    logger.warning("‚ö†Ô∏è Nincs adat, v√°rakoz√°s...")
                    time.sleep(self.update_interval)
                    continue

                # 2. √ìr√°nk√©nti mintav√©telez√©s
                hourly_data = self.sample_hourly_data(raw_data)

                # 3. Felt√∂lt√©sek elt√°vol√≠t√°sa
                cleaned_data = self.detect_refills(hourly_data)

                # 4. El≈ërejelz√©s
                prediction = self.calculate_prediction(cleaned_data)

                # 5. Szenzor friss√≠t√©se
                if prediction:
                    self.update_sensor(prediction)

                logger.info(f"‚è∞ K√∂vetkez≈ë friss√≠t√©s {self.update_interval} m√°sodperc m√∫lva...")
                time.sleep(self.update_interval)

            except Exception as e:
                logger.error(f"‚ùå Hiba a fut√°s sor√°n: {e}", exc_info=True)
                time.sleep(60)  # Hiba eset√©n 1 perc v√°rakoz√°s

if __name__ == '__main__':
    addon = SiloPredictionAddon()
    addon.run()
